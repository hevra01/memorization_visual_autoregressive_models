{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ðŸš€ For an interactive experience, head over to our [demo platform](https://var.vision/demo) and dive right in! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/data_mani_compress/work/miniforge3/envs/var_mem/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'memorization/checkpoints/vae_ch160v4096z32.pth', f'memorization/checkpoints/var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): \n",
    "    print(\"test\")\n",
    "    os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "ckpt = torch.load(var_ckpt, map_location='cpu')\n",
    "ckpt.pop('attn_bias_for_masking', None)\n",
    "var.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "#var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from memorization.data_prep.subset_imagenet import get_balanced_subset\n",
    "\n",
    "class IndexedDataset(torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Wraps an existing dataset so that __getitem__ returns (img, label, index).\n",
    "        The index refers to the position in the wrapped dataset (after subsetting).\n",
    "        \"\"\"\n",
    "        def __init__(self, dataset):\n",
    "            self.dataset = dataset\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img, label = self.dataset[idx]\n",
    "            return img, label, idx\n",
    "\n",
    "\n",
    "base_dataset = ImageFolder(\n",
    "    root=os.path.join(\"/scratch/inf0/user/mparcham/ILSVRC2012\", \"val_categorized\"),\n",
    "    transform=None,\n",
    ")\n",
    "dataset = get_balanced_subset(\n",
    "    dataset=base_dataset,\n",
    "    total_samples=12800,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# Wrap to expose indices\n",
    "dataset = IndexedDataset(dataset)\n",
    "\n",
    "# ---------------------------\n",
    "# Collate function (keeps PIL + index)\n",
    "# ---------------------------\n",
    "def collate_pil(batch):\n",
    "    \"\"\"\n",
    "    batch: List[(PIL.Image, int label, int index)]\n",
    "    returns:\n",
    "        images: List[PIL.Image]\n",
    "        labels: LongTensor (B,)\n",
    "        indices: LongTensor (B,)  # stable dataset indices\n",
    "    \"\"\"\n",
    "    imgs, labels, indices = zip(*batch)\n",
    "    return (\n",
    "        list(imgs),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "        torch.tensor(indices, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "# Use distributed sampler to split dataset per-rank. If not initialized, it\n",
    "# gracefully becomes a single-process full-range sampler.\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=collate_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from utils.data import normalize_01_into_pm1\n",
    "\n",
    "mid_px = round(1.125 * 256)\n",
    "aug_transform = T.Compose([\n",
    "    T.Resize(mid_px, interpolation=InterpolationMode.LANCZOS),\n",
    "    T.RandomCrop((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    normalize_01_into_pm1,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images len: 4, labels shape: torch.Size([4]), indices shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "images, labels, indices = next(iter(loader))\n",
    "print(f'images len: {len(images)}, labels shape: {labels.shape}, indices shape: {indices.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/scene_repre/work/VAR/models/var.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "# Prepare VAE embeddings for teacher-forcing\n",
    "images_aug = torch.stack([aug_transform(img) for img in images], dim=0).to(device)\n",
    "indices = var.vae_proxy[0].img_to_idxBl(images_aug)\n",
    "emb = torch.cat(\n",
    "    [var.vae_quant_proxy[0].embedding(idx) for idx in indices[1:]],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "# Forward pass (teacher forcing): hooks record activations\n",
    "_, test_atte = var(label_B=labels.to(device), x_BLCv_wo_first_l=emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 680, 680])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(test_atte.shape)\n",
    "print(test_atte[0][0][0][:10]) # first scale\n",
    "print(test_atte[0][0][1][:10]) # second scale\n",
    "print(test_atte[0][0][2][:10]) # second scale\n",
    "print(test_atte[0][0][3][:10]) # second scale\n",
    "print(test_atte[0][0][4][:10]) # second scale\n",
    "print(test_atte[0][0][5][:10]) # third scale\n",
    "print(test_atte[0][0][30][:20]) # third scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 9])\n",
      "torch.Size([4, 256])\n",
      "tensor([[1997, 1362, 3823,  ..., 3324, 4040, 3737],\n",
      "        [2741, 1917,  312,  ..., 1439, 2563, 1071],\n",
      "        [2211, 2241,  342,  ..., 1080, 1081,  297],\n",
      "        [3448, 1625,  860,  ..., 2183, 2804, 1052]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# a list of len 10, each of shape (B, num_patches), and each entry is an index to the codebook\n",
    "print(len(indices))\n",
    "print(indices[0].shape)\n",
    "print(indices[1].shape)\n",
    "print(indices[2].shape)\n",
    "print(indices[9].shape)\n",
    "print(indices[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 679, 32])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape) # torch.Size([4, 679, 32]), batch size 4, 679 tokens, embedding dim 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 680, 680])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0')\n",
      "tensor([-inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf], device='cuda:0')\n",
      "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(var.attn_bias_for_masking.shape)\n",
    "print(var.attn_bias_for_masking[0][0][0][:10]) # first scale\n",
    "print(var.attn_bias_for_masking[0][0][1][:10]) # second scale\n",
    "print(var.attn_bias_for_masking[0][0][2][:10]) # second scale\n",
    "print(var.attn_bias_for_masking[0][0][3][:10]) # second scale\n",
    "print(var.attn_bias_for_masking[0][0][4][:10]) # second scale\n",
    "print(var.attn_bias_for_masking[0][0][5][:10]) # third scale\n",
    "print(var.attn_bias_for_masking[0][0][30][:20]) # third scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "/BS/scene_repre/work/VAR/models/var.py\n",
      "tensor([0, 1, 2, 3, 4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(var.__init__.__code__.co_firstlineno)\n",
    "print(var.__init__.__code__.co_filename)\n",
    "\n",
    "mask = var.attn_bias_for_masking[0, 0]\n",
    "q = 30\n",
    "\n",
    "allowed = torch.isfinite(mask[q])\n",
    "print(torch.unique(var.lvl_1L.squeeze()[allowed]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([680])\n",
      "tensor([0, 1, 2, 3, 4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "q = 30  # pick a token index\n",
    "allowed = torch.isfinite(var.attn_bias_for_masking[0, 0, q])\n",
    "print(allowed.shape)\n",
    "print(torch.unique(var.lvl_1L.squeeze()[allowed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 680])\n"
     ]
    }
   ],
   "source": [
    "# d: torch.Size([1, 680, 1]) assigns level indices to each token\n",
    "d: torch.Tensor = torch.cat([torch.full((pn*pn,), i) for i, pn in enumerate(patch_nums)]).view(1, 680, 1)\n",
    "dT = d.transpose(1, 2) # torch.Size([1, 1, 680])    # dT: 11L\n",
    "lvl_1L = dT[:, 0].contiguous() # torch.Size([1, 680])\n",
    "print(lvl_1L.shape)\n",
    "# A query at level s can attend to any key from the same or earlier level.\n",
    "attn_bias_for_masking = torch.where(d >= dT, 0., -torch.inf).reshape(1, 1, 680, 680) # query_level â‰¥ key_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_lvl = (d == (dT + 1))                # keys at level s-1\n",
    "first_lvl = (d == 0) & (dT == 0)          # level-0 queries attend to level-0 keys\n",
    "mask_bool = prev_lvl | first_lvl\n",
    "attn_bias_for_masking = torch.where(mask_bool, 0., -torch.inf).reshape(1, 1, 680, 680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 680, 680])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([-inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf])\n",
      "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(attn_bias_for_masking.shape)\n",
    "print(attn_bias_for_masking[0][0][0][:10]) # first scale\n",
    "print(attn_bias_for_masking[0][0][1][:10]) # second scale\n",
    "print(attn_bias_for_masking[0][0][2][:10]) # second scale\n",
    "print(attn_bias_for_masking[0][0][3][:10]) # second scale\n",
    "print(attn_bias_for_masking[0][0][4][:10]) # second scale\n",
    "print(attn_bias_for_masking[0][0][5][:10]) # third scale\n",
    "print(attn_bias_for_masking[0][0][30][:20]) # third scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 680, 1])\n",
      "torch.Size([1, 1, 680])\n",
      "torch.Size([1, 680])\n"
     ]
    }
   ],
   "source": [
    "print(d.shape)\n",
    "print(d.transpose(1, 2).shape)\n",
    "print(var.lvl_1L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "         3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "         5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6,\n",
      "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "         9, 9, 9, 9, 9, 9, 9, 9]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(var.lvl_1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 5), (5, 14), (14, 30), (30, 55), (55, 91), (91, 155), (155, 255), (255, 424), (424, 680)]\n",
      "AdaLNSelfAttn(\n",
      "  shared_aln=False\n",
      "  (drop_path): Identity()\n",
      "  (attn): SelfAttention(\n",
      "    using_flash=False, using_xform=False, attn_l2_norm=True\n",
      "    (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (proj_drop): Identity()\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    fused_mlp_func=False\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU(approximate='tanh')\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (drop): Identity()\n",
      "  )\n",
      "  (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "  (ada_lin): Sequential(\n",
      "    (0): SiLU()\n",
      "    (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(var.begin_ends)\n",
    "print(var.blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)\n",
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "var_mem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
